
```markdown
# ğŸ¤Ÿ American Sign Language (ASL) Recognition System

This project presents a **real-time American Sign Language (ASL) recognition system** using **MediaPipe hand landmarks** and **Deep Learning**.  
The system detects hand gestures through a webcam, extracts landmark features, trains a neural network model, and predicts ASL alphabets in real time.

---

## ğŸ“Œ Project Overview
Communication barriers exist between hearing-impaired individuals and the general population.  
This project aims to reduce this gap by enabling **automatic recognition of ASL hand gestures** using computer vision and machine learning techniques.

---

## âœ¨ Features
- ğŸ¥ Real-time hand gesture detection
- âœ‹ MediaPipe-based hand landmark extraction (21 landmarks)
- ğŸ§  Deep Learning model for classification
- ğŸ”¤ ASL alphabet recognition
- âš¡ Fast and lightweight execution
- ğŸ’» Webcam-based live prediction

---

## ğŸ—‚ Project Structure
```

ASL/
â”‚â”€â”€ asl_dataset/            # Dataset (CSV landmark files)
â”‚   â”œâ”€â”€ A/
â”‚   â”œâ”€â”€ B/
â”‚   â””â”€â”€ C/
â”‚â”€â”€ venv/                   # Virtual environment
â”‚â”€â”€ collection.py           # Dataset collection
â”‚â”€â”€ train.py                # Model training
â”‚â”€â”€ predict.py              # Real-time prediction
â”‚â”€â”€ asl_model.h5             # Trained model
â”‚â”€â”€ README.md

````

---

## ğŸ›  Technologies Used
- Python
- OpenCV
- MediaPipe
- TensorFlow / Keras
- NumPy
- Scikit-learn

---

## âš™ï¸ System Workflow

### 1ï¸âƒ£ Data Collection
- Webcam captures hand gestures
- MediaPipe extracts **21 hand landmarks**
- Each landmark has **x, y, z coordinates**
- Data saved as CSV files (63 features per sample)

---

### 2ï¸âƒ£ Model Training
- CSV data is loaded and preprocessed
- Labels are encoded numerically
- Neural network is trained using Dense layers
- Trained model saved as `asl_model.h5`

---

### 3ï¸âƒ£ Real-time Prediction
- Webcam input processed frame-by-frame
- Hand landmarks extracted
- Model predicts ASL alphabet
- Output displayed live on screen

---

## ğŸš€ Installation & Setup

### ğŸ”¹ Step 1: Create Virtual Environment
```bash
python -m venv venv
venv\Scripts\activate
````

### ğŸ”¹ Step 2: Install Dependencies

```bash
pip install tensorflow opencv-python mediapipe numpy scikit-learn
```

---

## â–¶ï¸ How to Run the Project

### ğŸ“Œ Collect Dataset

```bash
python collection.py
```

* Press **S** â†’ Save hand gesture
* Press **N** â†’ Next letter
* Press **Q** â†’ Quit

---

### ğŸ“Œ Train the Model

```bash
python train.py
```

---

### ğŸ“Œ Run Real-time Prediction

```bash
python predict.py
```

* Show ASL gesture in front of webcam
* Prediction appears on screen
* Press **Q** to exit

---

## ğŸ“Š Model Architecture

* Input Layer: 63 features
* Hidden Layer 1: Dense (128 neurons, ReLU)
* Hidden Layer 2: Dense (64 neurons, ReLU)
* Output Layer: Softmax
* Optimizer: Adam
* Loss Function: Sparse Categorical Crossentropy

---

## ğŸ“ˆ Results

* Real-time ASL recognition achieved
* Accurate prediction for trained alphabets
* Low latency and smooth execution

---

## âš ï¸ Limitations

* Supports limited alphabets (A, B, C)
* Sensitive to lighting and hand orientation
* Background noise may affect detection

---

## ğŸ”® Future Scope

* Support full ASL alphabet (Aâ€“Z)
* Word and sentence formation
* LSTM / CNN-LSTM based temporal modeling
* Text-to-speech conversion
* Web deployment using Streamlit or Flask

---

## ğŸ“ Project Type

* Academic / Mini Project / Final Year Project
* Domain: Artificial Intelligence & Computer Vision

---

## ğŸ‘©â€ğŸ’» Author

**Mahek**
AI & Data Science Student

---

## ğŸ“œ License

This project is intended for educational and academic purposes only.

```
